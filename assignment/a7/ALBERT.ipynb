{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 7 - working with pre-trained BERT-based models\n",
        "\n",
        "Today, we will work with a Bert variant implementation from Hugging Face (https://huggingface.co/) specifically the TensorFlow version of ALBERT.\n",
        "\n",
        "This notebook does **NOT** require a GPU to run so you can use it in your existing GCP instance.\n",
        "\n",
        "This notebook requires their transformers library and the sentencepiece subword model.  Make sure you pip install them in your instance or run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers import AlbertTokenizer, TFAlbertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to look at the embeddings produced by the pre-trained model and examine your understanding of how BERT-based models work.\n",
        "\n",
        "Since we're only working with embeddings, will we need to create an output layer to make predictions?  No, we will not.  We can just use the raw outputs from ALBERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Your tensorflow version should be 2.6.2\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Your transformers version should be 4.15.0\n",
        "transformers.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentence = \"Children mark the inexorable march of time.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by tokenizing a sentence. All BERT-based models have their own tokenizers.  These are built based on the texts used in pre-training and are designed to minimize the number of 'UNK' tokens that will be encountered while putting a cap on the overall number of tokens in the vocabulary.  This means that words are often broken in to frequently occuring subwords.  During inference, previously unseen words can be built out of the subwords. \n",
        "    \n",
        "Albert has its own tokenizer and it needs to be used when you're working with an ALBERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's tokenize our input sentence to see how it gets broken up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.tokenize(test_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prefix '‚ñÅ' indicates a word boundary.  This allows the original input string to be reconstructed from the tokens. Note the word 'inexorable' has been broken into subwords.  Only the initial token has the prefix. Each token has an associated input embedding that gets passed in to the model. \n",
        "\n",
        "Let's do a short excercise to get familiar with BERT-based models. BERT gives us *contextualized embeddings*, i.e. embeddings for the same word in different contexts should be different. Let's see if it's true!\n",
        "\n",
        "Let's compare the context-based embedding vectors for '*mark*' in the following 6 sentences. In order to do so we first need to tokenize the input, which is very straighforward with the appropriate Hugging Face tokenizer.  \n",
        "\n",
        "How do we deal with the different sizes of the sentences? Hugging Face includes a padding argument that does it for us. The model calculates the max sentence length and pads other sentences to that length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "albert_inputs = tokenizer([\"Mark your calendars for the event\",\n",
        "                    \"It ended with a question mark\",\n",
        "                    \"Mark really enjoys teaching the W266 class\",\n",
        "                    \"Mark left a mark on the wall\",\n",
        "                    \"He left a mark on the professional literature\",\n",
        "                    \"They prefer the ride in a Lincoln Mark IV\" ],\n",
        "                  padding=True,\n",
        "                  return_tensors='tf')\n",
        "\n",
        "albert_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are actually three outputs: the token ids for the input sentences (starting with the '[CLS]' token by default), the token_type_ids which are useful when one has separate segments, and the attention masks which are used to mask out padding tokens.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Looking at the input_ids layer, what is the integer id for the '[CLS]' token?\n",
        "2. Looking at the input_ids layer, what is the integer id for the word 'mark'?\n",
        "\n",
        "Next, let's define a **Keras layer using the pre-trained ALBERT model** from Hugging Face. Make sure the model's name begins with 'TF' so we're usin the TensorFlow version!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "albert_layer = TFAlbertModel.from_pretrained('albert-base-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are just using the model as it was already trained (e.g. just using the embeddings that emerge from the top of the model) we can ignore the warning messages.\n",
        "\n",
        "Let's get the ALBERT encoding for all of our test sentences using the Functional API approach: \n",
        "\n",
        "layer_output = layer(layer_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "albert_outputs = albert_layer(albert_inputs)\n",
        "\n",
        "print('shape of token outputs: \\t\\t', albert_outputs[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first ALBERT output gets us the token-level embeddings. Let's define a function that shows the respective cosine distances between a list of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_distances(vecs):\n",
        "    for v_1 in vecs:\n",
        "        distances = ''\n",
        "        for v_2 in vecs:\n",
        "            distances += ('\\t' + str(np.dot(v_1, v_2)/np.sqrt(np.dot(v_1, v_1) * np.dot(v_2, v_2)))[:4])\n",
        "        print(distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we designate the 'mark'-token positions in the *encoded* input and extract the proper components: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mark_1 = albert_outputs[0][0, 1]\n",
        "mark_2 = albert_outputs[0][1, 6]\n",
        "mark_3 = albert_outputs[0][2, 1]\n",
        "mark_4 = albert_outputs[0][3, 1]\n",
        "mark_5 = albert_outputs[0][3, 4]\n",
        "mark_6 = albert_outputs[0][4, 4]\n",
        "mark_7 = albert_outputs[0][5, 9]\n",
        "\n",
        "marks = [mark_1, mark_2, mark_3, mark_4, mark_5, mark_6, mark_7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the pair-wise cosine distances in a table where the rows are the sentences and the columns are our mentions of the word 'mark':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cosine_distances(marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks rights! The name 'Mark' in the fourth sentence 'Mark left a mark on the wall' is similar to the embedding for the name in the third sentence but different from the embedding for the 'mark' on the wall.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "3. How are the embeddings contextualized by the model?\n",
        "\n",
        "4. Which sentence has a 'mark' *least* similar to the name 'Mark' in sentence three?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
