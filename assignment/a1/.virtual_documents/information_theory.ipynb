# Initial setup. Just run this cell once.
import numpy as np
import matplotlib.pyplot as plt
import warnings
get_ipython().run_line_magic("matplotlib", " inline")
warnings.filterwarnings('ignore')


def XLogX(x):
    """Returns x * log2(x)."""
    return np.nan_to_num(x * np.log2(x))

def BinaryEntropy(p):
    """Compute the entropy of a coin toss with P(heads) = p."""
    #### YOUR CODE HERE ####
    # Hint: Make sure you sum over all possible outcomes (heads & tails)!
    #### END YOUR CODE ####

# Let's try running it for p = 0.  This means the coin always comes up "tails".
# We expect that the entropy of this is 0 as there is no uncertainty about the outcome.
assert 0.0 == BinaryEntropy(0)

# We expect p = 0.5 to be as uncertain as it gets.  There's no good prior guess
# as to which of heads or tails the coin is going to come down on.
# As a result, we expect this to be bigger than p=0 above, but also bigger than any
# other value of p.
assert BinaryEntropy(0.5) > BinaryEntropy(0)
assert BinaryEntropy(0.5) > BinaryEntropy(0.49)
assert BinaryEntropy(0.5) > BinaryEntropy(0.51)

# As it turns out the entropy at p=0.5 is 1.0.
assert 1.0 == BinaryEntropy(0.5)

print(BinaryEntropy(0.73))


# Poking at a couple of individual values is interesting, but we can also simply plot
# entropy for all possibly values of P(H).
# As expected, the curve is maximum at p = 0.5 when the outcome is most uncertain
# and decreases to 0 as either heads or tails becomes a certainty.
p_of_heads = np.arange(0, 1.01, 0.01);
plt.plot(p_of_heads, BinaryEntropy(p_of_heads))
plt.xlabel('P(X = Heads)'); plt.ylabel('Entropy $H_2(X)$')
