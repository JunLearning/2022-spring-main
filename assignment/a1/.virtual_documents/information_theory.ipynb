# Initial setup. Just run this cell once.
import numpy as np
import matplotlib.pyplot as plt
import warnings
get_ipython().run_line_magic("matplotlib", " inline")
warnings.filterwarnings('ignore')


def XLogX(x):
    """Returns x * log2(x)."""
    return np.nan_to_num(x * np.log2(x))

def BinaryEntropy(p):
    """Compute the entropy of a coin toss with P(heads) = p."""
    #### YOUR CODE HERE ####
    # Hint: Make sure you sum over all possible outcomes (heads & tails)!
    return -(XLogX(p) + XLogX(1-p))
    #### END YOUR CODE ####

# Let's try running it for p = 0.  This means the coin always comes up "tails".
# We expect that the entropy of this is 0 as there is no uncertainty about the outcome.
assert 0.0 == BinaryEntropy(0)

# We expect p = 0.5 to be as uncertain as it gets.  There's no good prior guess
# as to which of heads or tails the coin is going to come down on.
# As a result, we expect this to be bigger than p=0 above, but also bigger than any
# other value of p.
assert BinaryEntropy(0.5) > BinaryEntropy(0)
assert BinaryEntropy(0.5) > BinaryEntropy(0.49)
assert BinaryEntropy(0.5) > BinaryEntropy(0.51)

# As it turns out the entropy at p=0.5 is 1.0.
assert 1.0 == BinaryEntropy(0.5)

print(BinaryEntropy(0.73))


# Poking at a couple of individual values is interesting, but we can also simply plot
# entropy for all possibly values of P(H).
# As expected, the curve is maximum at p = 0.5 when the outcome is most uncertain
# and decreases to 0 as either heads or tails becomes a certainty.
p_of_heads = np.arange(0, 1.01, 0.01);
plt.plot(p_of_heads, BinaryEntropy(p_of_heads))
plt.xlabel('P(X = Heads)'); plt.ylabel('Entropy $H_2(X)$')


from math import log
def pmi(p_x, p_y, p_xy):
    return log(p_xy / (p_x * p_y), 2)
p_rc = 0.1; p_r = 0.1; p_c = 0.71
print(f'1. {pmi(p_r, p_c, p_rc)}')
p_w = 0.009; p_p = 0.013; p_wp = 0.003
print(f'2. {pmi(p_w, p_p, p_wp)}')


print(f'1. {-log(1/128, 2)} | {-log(1/1024, 2)}')
print(f'2. (A) has higher entropy, because of uniform distribution without context.')
print(f'3. (A) has higher entropy becuase of higher standard deviation.')


# 1
def ce(yi, yi_hat):
    return np.mean(yi * np.nan_to_num((-np.log2(yi_hat))))
yi = [1, 0, 0, 0]; yi_hat = [0.53,0.36,0.055,0.055]
print(f'1. CrossEntropy: {format(ce(yi, yi_hat), "0.5f")}')
print(f'2. D_KL = {ce(yi, yi_hat)} becuase entropy is 0')
print(f"3. entropy is 0 in this case, don't need to compute D_KL")
print(f"4. 0")
yi_hat5 = [0, 1, 0, 0]
print(f"5. {ce(yi, yi_hat5)}")
yi_hat6 = [0, 1/3, 1/3, 1/3]
print(f"6. {ce(yi, yi_hat6)}")



