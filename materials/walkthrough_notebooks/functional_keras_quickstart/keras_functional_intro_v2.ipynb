{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Keras functional API \n",
    "\n",
    "<a id = 'top'></a>\n",
    "\n",
    "  * A. [Intro to Functional API](#introToFunctionalKeras) \n",
    "  * B. [A word about word embeddings](#wordAboutEmbeddings)\n",
    "  * C. [Description of our dataset](#datasetDescription) \n",
    "  * D. [Let's Build our functional model](#buildModel)\n",
    "  * E. [Let's Train and Evaluate our model](#trainModel)\n",
    "  * F. [Considerations for further exploration](#furtherExploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'introToFunctionalKeras'></a>\n",
    "## A. Intro to Functional API\n",
    "The Keras functional syntax for models is only slightly different from the sequential syntax.  It is an intuitive way to build up model graphs.  In return it offers much greater flexibility, allowing multiple connections from arbitrary layers to other layers.  \n",
    "\n",
    "It is referred to as \"functional\" since you use layers as functions called on input nodes to create output nodes.  A good place to start is the [Keras functional documentation](https://keras.io/guides/functional_api/)\n",
    "\n",
    "Many of the more advanced deep-learning networks you will learn about require layer connections which are not strictly sequential; therefore we will mostly use the functional syntax moving forward.\n",
    "\n",
    "This notebook gives you a sense of how to use the functional syntax to create a network, even though our example model will end up being one where layers are consecutively connected.\n",
    "\n",
    "[Go to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.data import find\n",
    "\n",
    "#VERY IMPORTANT - the gensim code here requires gensim==4.0.1.\n",
    "#make sure you pip install that version of the code.  This will be updated in the future.\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'wordAboutEmbeddings'></a>\n",
    "## B. A word about word embeddings\n",
    "\n",
    "This example will use pre-trained word embeddings from [nltk / gensim](https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest) \n",
    "\n",
    "You will have more chance to work with embedding vectors in a later homework.  If you haven't been exposed to word embeddings (like \"word2vec\") think of embeddings as a way to represent word meaning in geometric space, allowing us to do things like compare word similarities using vector operations like dot products.  For now, we'll use the word2vec pre-trained model to create an embedding layer for our classifier model.\n",
    "\n",
    "[Go to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/jun/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of gensim embedding vocabulary : 43981\n",
      "Length d of embedding vectors : 300\n",
      "The shape of our embedding matrix is (43982, 300)\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK's sample word2vec embeddings\n",
    "\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "\n",
    "embmod = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "print('The length of gensim embedding vocabulary : {}'.format(len(embmod)))\n",
    "EMBEDDING_DIM = len(embmod['university'])\n",
    "print('Length d of embedding vectors : {}'.format(EMBEDDING_DIM))\n",
    "\n",
    "# initialize embedding matrix and word-to-id map:\n",
    "embedding_matrix = np.zeros((len(embmod) + 1, EMBEDDING_DIM))       \n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(embmod.key_to_index.keys()):\n",
    "    embedding_vector = embmod[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n",
    "print('The shape of our embedding matrix is {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'datasetDescription'></a>\n",
    "## C. Description of our dataset\n",
    "\n",
    "We will create a classifier model on the [large movie review dataset from Stanford AI Lab](https://ai.stanford.edu/~amaas/data/sentiment/). \n",
    "\n",
    "This dataset was used in the paper:  \n",
    "[Learning Word Vectors for Sentiment Analysis by Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher Potts from the Jun 2011 issue of ACL.](https://www.aclweb.org/anthology/P11-1015.pdf)  \n",
    "\n",
    "The dataset training samples are split between:\n",
    "  * \"negative\" movie reviews, with ratings in the range (1,2,3,4) and \n",
    "  * \"positive reviews in the range of (7,8,9,10).\n",
    " \n",
    "We will re-group the ratings into 2 \"negative\" groups and 2 \"positive\" groups, to make our classification task a bit easier.\n",
    "\n",
    "[Go to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading file\n",
      "Finished untar-ing file\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'4162_10 2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dp/f69vs__d2xl1hb9zdnxjh2k80000gn/T/ipykernel_8616/1988417536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpos_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Extract label from filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mpos_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaplabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mneg_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '4162_10 2.txt'"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "import requests\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "print('Finished downloading file')\n",
    "open('aclImdb_v1.tar.gz', 'wb').write(r.content)\n",
    "\n",
    "# Untar the downloaded file\n",
    "!tar xfz aclImdb_v1.tar.gz\n",
    "print('Finished untar-ing file')\n",
    "\n",
    "input_length = 100\n",
    "import os\n",
    "import re\n",
    "pos_directory = \"./aclImdb/train/pos/\"\n",
    "neg_directory = \"./aclImdb/train/neg/\"\n",
    "\n",
    "# The original labels are negative in [1,2,3,4] and 'positive in [7,8,9,10]'\n",
    "# To simplify our classification let's reduce the number of classes by remapping ratings:\n",
    "#     Map labels to new labels\n",
    "#      1, 2  map to 0\n",
    "#      3, 4 map to 1\n",
    "#      7, 8  map to 2\n",
    "#      9, 10 map to 3\n",
    "maplabels = {\"1\" : 0, \"2\" : 0, \n",
    "             \"3\" : 1, \"4\" : 1,\n",
    "             \"7\" : 2, \"8\" : 2,\n",
    "             \"9\" : 3, \"10\" : 3}\n",
    "\n",
    "# Create pos and neg training samples by tokenizing the first 100 words from each review\n",
    "pos_samples = []\n",
    "pos_labels = []\n",
    "\n",
    "pat = re.compile('\\d+_(\\d+).txt')\n",
    "for file in os.listdir(pos_directory):\n",
    "    with open(pos_directory + file, 'r') as fh:\n",
    "        review = fh.readline().split()\n",
    "        if len(review) < input_length:\n",
    "            whole = int(input_length / len(review) )\n",
    "            leftover = len(review) % input_length\n",
    "            review = review * whole + review[:leftover]\n",
    "        tokens = [vocab_dict.get(word, 43981) for word in review[:input_length]] # only take the first input_length words\n",
    "        pos_samples.append(tokens)\n",
    "        label = pat.sub(r'\\1', file) # Extract label from filename\n",
    "        pos_labels.append(maplabels[label])\n",
    "        \n",
    "neg_samples = []\n",
    "neg_labels = []\n",
    "for file in os.listdir(neg_directory):\n",
    "    with open(neg_directory + file, 'r') as fh:\n",
    "        review = fh.readline().split()\n",
    "        if len(review) < input_length:\n",
    "            whole = int(input_length / len(review) )\n",
    "            leftover = len(review) % input_length\n",
    "            review = review * whole + review[:leftover]\n",
    "        tokens = [vocab_dict.get(word, 43981) for word in review[:input_length]]\n",
    "        neg_samples.append(tokens)\n",
    "        label = pat.sub(r'\\1', file) # Extract label from filename\n",
    "        neg_labels.append(maplabels[label])\n",
    "        \n",
    "# Create x samples and labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xsamples = pos_samples + neg_samples # combine lists\n",
    "# labels = [1] * len(pos_samples) + [0] * len(neg_samples)\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "# Repackage as numpy arrays\n",
    "xsamples = np.array(xsamples)\n",
    "labels = np.array(labels).reshape([-1,1]) # Labels must have at least one column\n",
    "\n",
    "# Create training, eval split\n",
    "train_input, test_input, train_labels, test_labels = train_test_split(xsamples, labels, test_size = 0.2, \n",
    "                                                         random_state = 41,\n",
    "                                                         shuffle = True)\n",
    "\n",
    "# Investigate the unique class \"levels\"\n",
    "print('Our unique class labels are: {}'.format(np.unique(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look for possible class imbalance\n",
    "ltrain = len(train_labels)\n",
    "print('Number of training samples = {}'.format(ltrain))\n",
    "for i in range(4):\n",
    "    nclass = len(np.where(train_labels.reshape([-1]) == i)[0])\n",
    "    print('class ' + str(i) + ' has {} samples ({:.2f}%)'.format(nclass, 100 * float(nclass / ltrain )))\n",
    "\n",
    "print()\n",
    "ltest = len(test_labels)\n",
    "print('Number of test samples = {}'.format(ltest))\n",
    "for i in range(4):\n",
    "    nclass = len(np.where(test_labels.reshape([-1]) == i)[0])\n",
    "    print('class ' + str(i) + ' has {} samples ({:.2f}%)'.format(nclass, 100 * float(nclass / ltest )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'buildModel'></a>\n",
    "## D. Let's build our functional model\n",
    "Building up a graph involves using layers as functions and calling those functions on input tensors to create output tensors, which then can be used to feed other layers.\n",
    "\n",
    "For example, you can create an input and output connection using a dense layer by doing the following:\n",
    "\n",
    "  * x = Dense(50, activation='relu')(x)\n",
    "\n",
    "or by declaring the layer separately and calling that layer as a function with input, x:\n",
    "\n",
    "  * dlayer = Dense(50, activation='relu')  \n",
    "  * x = dlayer(x)\n",
    "\n",
    "In the second representation, you can see that layer instances resemble functions called on graph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#  Package our word2vec embedding model as a fixed pre-trained embedding.\n",
    "#  Note we set the argument \"trainable\" to False as we don't want to modify our pre-trained\n",
    "#  ...embedding values during model training.\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            # input_length=input_length,\n",
    "                            trainable=False)\n",
    "\n",
    "# Note that \"trainable\" is set to False, since we want our classifier model to leave embedding vectors alone during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our first embedding layer, let's build the model adding other layers using the **Functional API**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a word length to truncate each movie review\n",
    "input_length = 100\n",
    "\n",
    "# Define an input using our input fixed number of words per review sample\n",
    "input_x = Input(shape = (input_length,), name=\"input_words\")\n",
    "\n",
    "# Apply embeddings to each word input\n",
    "x1 = embedding_layer(input_x) \n",
    "\n",
    "# Use Keras lambda function layer to take the mean of the 100 word vectors\n",
    "x2 = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1))(x1)    # average of embedding vectors\n",
    "\n",
    "# Continue building the network graph                                \n",
    "x3 = Dense(50, activation='relu', name=\"hidden1\")(x2)   # hidden layer\n",
    "x4 = Dense(20, activation='relu', name=\"hidden2\")(x3)   # hidden layer\n",
    "\n",
    "# Our final output classifier layer.  It has 4 outputs (one for each class)\n",
    "# Because this is a multi-class output we'll use softmax as an activation.\n",
    "# We want the predictor outputs to always sum to 1 to represent probability.\n",
    "yhat = Dense(4, activation='softmax', name=\"output\")(x4)   # classification layer\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_x, outputs=yhat, name=\"keras_func_model\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review the output layers, output dimensions, number of weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether our dimension discussion was correct. Print a model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'trainModel'></a>\n",
    "## E. Let's Train and Evaluate our model\n",
    "\n",
    "[Return to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_input, train_labels, batch_size = 400, validation_split=0.2, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss for training / validation samples\n",
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('sparse_categorical_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'furtherExploration'></a>\n",
    "## F. Some considerations for further exploration are:\n",
    "1. What evidence do you see that the above model might be over-fitting?\n",
    "\n",
    "2. Might BOW have some basic limitations for classification accuracy?  You'll explore this more in a homework on a BOW classifier.\n",
    "\n",
    "3. How might class imbalance impact possible accuracy / bias a model in a particular way to impact accuracy?  How could this be overcome?\n",
    "\n",
    "4. We are sparse_crossentropy_loss as loss function.  This might not be the optimal loss function.  Can you think of why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
